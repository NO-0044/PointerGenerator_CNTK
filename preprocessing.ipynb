{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing for original CNN\\DM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"]\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_missing_period(line):\n",
    "    if \"@highlight\" in line: \n",
    "        return line\n",
    "    if line==\"\": \n",
    "        return line\n",
    "    if line[-1] in END_TOKENS: \n",
    "        return line\n",
    "    return line + \" .\"\n",
    "\n",
    "def get_art_abs(lines):\n",
    "    lines = [line.lower() for line in lines]\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "    for idx,line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "    article = ' '.join(article_lines)\n",
    "    abstract = '. '.join(highlights)\n",
    "    return article, abstract\n",
    "\n",
    "def readlines(file):\n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc = Counter()\n",
    "with open('data/stories.tsv','w',encoding='utf-8') as fout:\n",
    "    for path in ['data/cnn_stories_tokenized/cnn_stories_tokenized/','data/dm_stories_tokenized/dm_stories_tokenized/']:\n",
    "        for file in os.listdir(path):\n",
    "            lines = readlines(os.path.join(path,file))\n",
    "            article, abstract = get_art_abs(lines)\n",
    "            if article.strip() == \"\" or abstract.strip() == \"\":\n",
    "                continue\n",
    "            art_tokens = article.split(' ')\n",
    "            abs_tokens = abstract.split(' ')\n",
    "            tokens = art_tokens + abs_tokens\n",
    "            tokens = [t.strip() for t in tokens]\n",
    "            tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "            voc.update(tokens)\n",
    "            fout.write('<s> %s </s>\\t<s> %s </s>\\n'%(article,abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/origin_voc.pickle','wb') as f:\n",
    "    pickle.dump(voc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashhex(s):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s.encode())\n",
    "    return h.hexdigest()\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "    return [hashhex(url) for url in url_list]\n",
    "\n",
    "urllist = {}\n",
    "for a in ['cnn','dailymail']:\n",
    "    for b in ['training','test','validation']:\n",
    "        with open('data/url_lists/'+a+'_wayback_'+b+'_urls.txt','r') as f:\n",
    "            tmp = []\n",
    "            for line in f:\n",
    "                tmp.append(line.strip())\n",
    "            urllist[a+b] = get_url_hashes(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ctf_write(f,article,abstract,id):\n",
    "    article = '<s> '+article.strip()+' </s>'\n",
    "    abstract = '<s> '+abstract.strip()+' </s>'\n",
    "    article_words = article.split()\n",
    "    article_input = []\n",
    "    article_extended_input = []\n",
    "    abstract_words = abstract.split()\n",
    "    abstract_input = []\n",
    "    abstract_extended_input = []\n",
    "    oov = {}\n",
    "    for i,w in enumerate(article_words):\n",
    "        if i == 400:\n",
    "            article_input.append(voc['</s>'])\n",
    "            article_extended_input.append(voc['</s>'])\n",
    "            break\n",
    "        try:\n",
    "            article_input.append(voc[w])\n",
    "            article_extended_input.append(voc[w])\n",
    "        except KeyError:\n",
    "            oov[w] = len(voc)+len(oov)\n",
    "            article_input.append(voc['<unk>'])\n",
    "            article_extended_input.append(oov[w])\n",
    "    for i,w in enumerate(abstract_words):\n",
    "        if i == 100:\n",
    "            abstract_input.append(voc['</s>'])\n",
    "            abstract_extended_input.append(voc['</s>'])\n",
    "            break\n",
    "        try:\n",
    "            abstract_input.append(voc[w])\n",
    "            abstract_extended_input.append(voc[w])\n",
    "        except KeyError:\n",
    "            abstract_input.append(voc['<unk>'])\n",
    "            try:\n",
    "                abstract_extended_input.append(oov[w])\n",
    "            except KeyError:\n",
    "                abstract_extended_input.append(voc['<unk>'])\n",
    "    for i,wid in enumerate(article_input):\n",
    "        if i < len(abstract_extended_input):\n",
    "            f.write(\"%d\\t|S0 %d:1\\t|S1 %d:1\\t|S2 %d:1\\t|S3 %d:1\\n\"%(id,wid,article_extended_input[i],abstract_input[i],abstract_extended_input[i]))\n",
    "            #f.write(\"%d\\t|S0 %d:1\\t|S1 %d:1\\t|S2 %d:1\\n\"%(id,wid,article_extended_input[i],abstract_extended_input[i]))\n",
    "        else:\n",
    "            f.write(\"%d\\t|S0 %d:1\\t|S1 %d:1\\n\"%(id,wid,article_extended_input[i]))\n",
    "    return len(article_input),len(abstract_extended_input),len(oov)\n",
    "\n",
    "voc = {}\n",
    "with open('./data/SELF_DATA/voc_50k.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        w,id = line.split('\\t')\n",
    "        voc[w] = int(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extended = []\n",
    "for b in ['training','test','validation']:\n",
    "    i = 0\n",
    "    print('create '+b)\n",
    "    with open('data/stories_'+b+'.ctf','w') as fout:\n",
    "        for url in urllist['cnn'+b]:\n",
    "            lines = readlines('data/cnn_stories_tokenized/cnn_stories_tokenized/'+url+'.story')\n",
    "            article, abstract = get_art_abs(lines)\n",
    "            if article.strip() == \"\" or abstract.strip() == \"\":\n",
    "                continue\n",
    "            extended.append(ctf_write(fout,article,abstract,i))\n",
    "            i += 1\n",
    "            #break\n",
    "        for url in urllist['dailymail'+b]:\n",
    "            lines = readlines('data/dm_stories_tokenized/dm_stories_tokenized/'+url+'.story')\n",
    "            article, abstract = get_art_abs(lines)\n",
    "            if article.strip() == \"\" or abstract.strip() == \"\":\n",
    "                continue\n",
    "            extended.append(ctf_write(fout,article,abstract,i))\n",
    "            i += 1\n",
    "            #break\n",
    "    print('%d %s data created'%(i,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Preprocessing for MSRA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Counter()\n",
    "for extend in ['desc','headline']:\n",
    "    with open('./data/SELF_DATA/train.'+extend,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(' ')\n",
    "            voc.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/SELF_DATA/voc_50k.txt','w',encoding='utf-8') as f:\n",
    "    #f.write('<unk>\\t0\\n')\n",
    "    f.write('<s>\\t0\\n')\n",
    "    f.write('</s>\\t1\\n')\n",
    "    f.write('<pad>\\t2\\n')\n",
    "    i = 3\n",
    "    for w in voc.most_common(50000):\n",
    "        f.write('%s\\t%d\\n'%(w[0],i))\n",
    "        i += 1\n",
    "    f.write('<unk>\\t%d\\n'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create test\n",
      "9961 test data created, total 701716 article tokens, 108871 headline tokens\n",
      "create valid\n",
      "10000 valid data created, total 704482 article tokens, 109053 headline tokens\n",
      "create train\n",
      "9094344 train data created, total 638924687 article tokens, 99021828 headline tokens\n"
     ]
    }
   ],
   "source": [
    "extended = []\n",
    "for a in ['test','valid','train']:\n",
    "    with open('data/SELF_DATA/%s.desc'%a,'r',encoding='utf-8') as desc_in:\n",
    "        with open('data/SELF_DATA/%s.headline'%a,'r',encoding='utf-8') as head_in:\n",
    "            with open('data/SELF_DATA/%s.ctf'%a,'w') as fout:\n",
    "                i = 0\n",
    "                article_tokens = 0\n",
    "                head_tokens = 0\n",
    "                print('create '+a)\n",
    "                for desc,head in zip(desc_in,head_in):\n",
    "                    tmp1,tmp2,tmp3 = ctf_write(fout,desc,head,i)\n",
    "                    article_tokens += tmp1\n",
    "                    head_tokens += tmp2\n",
    "                    extended.append(tmp3)\n",
    "                    i += 1\n",
    "                print('%d %s data created, total %d article tokens, %d headline tokens'%(i,a,article_tokens,head_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
