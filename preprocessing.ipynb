{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"]\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_period(line):\n",
    "    if \"@highlight\" in line: \n",
    "        return line\n",
    "    if line==\"\": \n",
    "        return line\n",
    "    if line[-1] in END_TOKENS: \n",
    "        return line\n",
    "    return line + \" .\"\n",
    "\n",
    "def get_art_abs(lines):\n",
    "    lines = [line.lower() for line in lines]\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "    for idx,line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "    article = ' '.join(article_lines)\n",
    "    abstract = '. '.join(highlights)\n",
    "    return article, abstract\n",
    "\n",
    "def readlines(file):\n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc = Counter()\n",
    "with open('data/stories.tsv','w',encoding='utf-8') as fout:\n",
    "    for path in ['data/cnn_stories_tokenized/cnn_stories_tokenized/','data/dm_stories_tokenized/dm_stories_tokenized/']:\n",
    "        for file in os.listdir(path):\n",
    "            lines = readlines(os.path.join(path,file))\n",
    "            article, abstract = get_art_abs(lines)\n",
    "            if article.strip() == \"\" or abstract.strip() == \"\":\n",
    "                continue\n",
    "            art_tokens = article.split(' ')\n",
    "            abs_tokens = abstract.split(' ')\n",
    "            tokens = art_tokens + abs_tokens\n",
    "            tokens = [t.strip() for t in tokens]\n",
    "            tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "            voc.update(tokens)\n",
    "            fout.write('<s> %s </s>\\t<s> %s </s>\\n'%(article,abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/origin_voc.pickle','rb') as f:\n",
    "    voc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/voc_50k.txt','w',encoding='utf-8') as f:\n",
    "    #f.write('<unk>\\t0\\n')\n",
    "    f.write('<s>\\t0\\n')\n",
    "    f.write('</s>\\t1\\n')\n",
    "    f.write('<pad>\\t2\\n')\n",
    "    i = 3\n",
    "    for w in voc.most_common(50000):\n",
    "        f.write('%s\\t%d\\n'%(w[0],i))\n",
    "        i += 1\n",
    "    f.write('<unk>\\t%d\\n'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashhex(s):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s.encode())\n",
    "    return h.hexdigest()\n",
    "\n",
    "def get_url_hashes(url_list):\n",
    "    return [hashhex(url) for url in url_list]\n",
    "\n",
    "urllist = {}\n",
    "for a in ['cnn','dailymail']:\n",
    "    for b in ['training','test','validation']:\n",
    "        with open('data/url_lists/'+a+'_wayback_'+b+'_urls.txt','r') as f:\n",
    "            tmp = []\n",
    "            for line in f:\n",
    "                tmp.append(line.strip())\n",
    "            urllist[a+b] = get_url_hashes(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create training\n",
      "287113 training data created\n",
      "create test\n",
      "11490 test data created\n",
      "create validation\n",
      "13368 validation data created\n"
     ]
    }
   ],
   "source": [
    "def ctf_write(f,article,abstract,id):\n",
    "    article = '<s> '+article+' </s>'\n",
    "    abstract = '<s> '+abstract+' </s>'\n",
    "    article_words = article.split()\n",
    "    article_input = []\n",
    "    article_extended_input = []\n",
    "    abstract_words = abstract.split()\n",
    "    abstract_input = []\n",
    "    abstract_extended_input = []\n",
    "    oov = {}\n",
    "    for i,w in enumerate(article_words):\n",
    "        if i == 400:\n",
    "            article_input.append(voc['</s>'])\n",
    "            article_extended_input.append(voc['</s>'])\n",
    "            break\n",
    "        try:\n",
    "            article_input.append(voc[w])\n",
    "            article_extended_input.append(voc[w])\n",
    "        except KeyError:\n",
    "            oov[w] = len(voc)+len(oov)\n",
    "            article_input.append(voc['<unk>'])\n",
    "            article_extended_input.append(oov[w])\n",
    "    for i,w in enumerate(abstract_words):\n",
    "        if i == 400:\n",
    "            abstract_input.append(voc['</s>'])\n",
    "            abstract_extended_input.append(voc['</s>'])\n",
    "            break\n",
    "        try:\n",
    "            abstract_input.append(voc[w])\n",
    "            abstract_extended_input.append(voc[w])\n",
    "        except KeyError:\n",
    "            abstract_input.append(voc['<unk>'])\n",
    "            try:\n",
    "                abstract_extended_input.append(oov[w])\n",
    "            except KeyError:\n",
    "                abstract_extended_input.append(voc['<unk>'])\n",
    "    for i,wid in enumerate(article_input):\n",
    "        if i < len(abstract_extended_input):\n",
    "            f.write(\"%d\\t|S0 %d:1\\t|S1 %d:1\\t|S2 %d:1\\t|S3 %d:1\\n\"%(id,wid,article_extended_input[i],abstract_input[i],abstract_extended_input[i]))\n",
    "            #f.write(\"%d\\t|S0 %d:1\\t|S1 %d:1\\t|S2 %d:1\\n\"%(id,wid,article_extended_input[i],abstract_extended_input[i]))\n",
    "        else:\n",
    "            f.write(\"%d\\t|S0 %d:1\\t|S1 %d:1\\n\"%(id,wid,article_extended_input[i]))\n",
    "    return len(oov)\n",
    "\n",
    "voc = {}\n",
    "with open('data/voc_50k.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        w,id = line.split('\\t')\n",
    "        voc[w] = int(id)\n",
    "\n",
    "extended = []\n",
    "for b in ['training','test','validation']:\n",
    "    i = 0\n",
    "    tmp_extended = 0\n",
    "    print('create '+b)\n",
    "    with open('data/stories_'+b+'.ctf','w') as fout:\n",
    "        for url in urllist['cnn'+b]:\n",
    "            lines = readlines('data/cnn_stories_tokenized/cnn_stories_tokenized/'+url+'.story')\n",
    "            article, abstract = get_art_abs(lines)\n",
    "            if article.strip() == \"\" or abstract.strip() == \"\":\n",
    "                continue\n",
    "            extended.append(ctf_write(fout,article,abstract,i))\n",
    "            i += 1\n",
    "            #break\n",
    "        for url in urllist['dailymail'+b]:\n",
    "            lines = readlines('data/dm_stories_tokenized/dm_stories_tokenized/'+url+'.story')\n",
    "            article, abstract = get_art_abs(lines)\n",
    "            if article.strip() == \"\" or abstract.strip() == \"\":\n",
    "                continue\n",
    "            extended.append(ctf_write(fout,article,abstract,i))\n",
    "            i += 1\n",
    "            #break\n",
    "    print('%d %s data created'%(i,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50003"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
