{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "attention_dim = 64\n",
    "use_attention = True\n",
    "use_embedding = True\n",
    "embedding_dim = 100\n",
    "length_increase = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(path):\n",
    "    vocab = [w.strip().split('\\t') for w in open(path,encoding='utf-8').readlines()]\n",
    "    i2w = { int(i):w for w,i in vocab }\n",
    "    w2i = { w:int(i) for w,i in vocab }\n",
    "    \n",
    "    return (vocab, i2w, w2i)\n",
    "\n",
    "vocab, i2w, w2i = get_vocab('data/voc_50k.txt')\n",
    "vocab = [x[0] for x in vocab]\n",
    "vocab_dim = len(vocab)\n",
    "max_extended_vocab_dim = vocab_dim+400\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        en_in = C.io.StreamDef(field='S0', shape=vocab_dim, is_sparse=True),\n",
    "        en_in_extended = C.io.StreamDef(field='S1', shape=max_extended_vocab_dim, is_sparse=True),\n",
    "        target = C.io.StreamDef(field='S2',shape=max_extended_vocab_dim, is_sparse=True)#,\n",
    "        #target = C.io.StreamDef(field='S3', shape=max_extended_vocab_dim, is_sparse=True)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "train_reader = create_reader('data/stories_test.ctf', True)\n",
    "\n",
    "#valid_reader = create_reader('data/stories_validation.ctf', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_start =C.Constant(np.array([i==w2i['<s>'] for i in range(max_extended_vocab_dim)], dtype=np.float32))\n",
    "sentence_end_index = vocab.index('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enAxis = C.Axis('enAxis')\n",
    "deAxis = C.Axis('deAxis')\n",
    "\n",
    "EncoderSequence = C.layers.SequenceOver[enAxis]\n",
    "DecoderSequence = C.layers.SequenceOver[deAxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_train(s2smodel):\n",
    "    # model used in training (history is known from labels)\n",
    "    # note: the labels must NOT contain the initial <s>\n",
    "    @C.Function\n",
    "    def model_train(input, extended_input, de): # (input*, labels*) --> (word_logp*)\n",
    "\n",
    "        # The input to the decoder always starts with the special label sequence start token.\n",
    "        # Then, use the previous value of the label sequence (for training) or the output (for execution).\n",
    "        past_labels = C.layers.Delay(initial_state=sentence_start)(de)\n",
    "        return s2smodel(past_labels, input, extended_input)\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model):\n",
    "    @C.Function\n",
    "    @C.layers.Signature(input=EncoderSequence[C.layers.Tensor[vocab_dim]],\n",
    "                        extended_input=EncoderSequence[C.layers.Tensor[max_extended_vocab_dim]],\n",
    "                        #de_input=DecoderSequence[C.layers.Tensor[vocab_dim]],\n",
    "                        target=DecoderSequence[C.layers.Tensor[max_extended_vocab_dim]])\n",
    "    def criterion(input, extended_input, target):#, de_input, target):\n",
    "        # criterion function must drop the <s> from the labels\n",
    "        postprocessed_target = C.sequence.slice(target,1 ,0)\n",
    "        z = model(input, extended_input, postprocessed_target)\n",
    "        print(repr(z))\n",
    "        ce = C.negate(C.reduce_sum(C.element_times(postprocessed_target,C.log(z[0]),name='loss'), axis=-1))\n",
    "        #ce = C.sequence.reduce_sum(C.sequence.gather(z[0], postprocessed_labels))\n",
    "        #ce = C.cross_entropy_with_softmax(z[0], postprocessed_target)\n",
    "        #errs = C.classification_error(z[0], postprocessed_target)\n",
    "        print(repr(ce))\n",
    "        return ce\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_reader, valid_reader, vocab, i2w, s2smodel, max_epochs, epoch_size):\n",
    "\n",
    "    # create the training wrapper for the s2smodel, as well as the criterion function\n",
    "    model_train = create_model_train(s2smodel)\n",
    "    criterion = create_criterion_function(model_train)\n",
    "\n",
    "    # also wire in a greedy decoder so that we can properly log progress on a validation example\n",
    "    # This is not used for the actual training process.\n",
    "    #model_greedy = create_model_greedy(s2smodel)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    minibatch_size = 1\n",
    "    lr = 0.001 if use_attention else 0.005\n",
    "    learner = C.fsadagrad(model_train.parameters,\n",
    "                          #apply the learning rate as if it is a minibatch of size 1\n",
    "                          lr = C.learning_parameter_schedule_per_sample([lr]*2+[lr/2]*3+[lr/4], epoch_size),\n",
    "                          momentum = C.momentum_schedule(0.9366416204111472, minibatch_size=minibatch_size),\n",
    "                          gradient_clipping_threshold_per_sample=2.3,\n",
    "                          gradient_clipping_with_truncation=True)\n",
    "    trainer = C.Trainer(None, criterion, learner)\n",
    "\n",
    "    # Get minibatches of sequences to train with and perform model training\n",
    "    total_samples = 0\n",
    "    mbs = 0\n",
    "    eval_freq = 100\n",
    "\n",
    "    # print out some useful training information\n",
    "    C.logging.log_number_of_parameters(model_train) ; print()\n",
    "    progress_printer = C.logging.ProgressPrinter(freq=30, tag='Training')    \n",
    "\n",
    "    # a hack to allow us to print sparse vectors\n",
    "    #sparse_to_dense = create_sparse_to_dense(input_vocab_dim)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        while total_samples < (epoch+1) * epoch_size:\n",
    "            # get next minibatch of training data\n",
    "            mb_train = train_reader.next_minibatch(minibatch_size)\n",
    "            print(mb_train[train_reader.streams.en_in].shape)\n",
    "            \n",
    "            # do the training\n",
    "            trainer.train_minibatch({criterion.arguments[0]: mb_train[train_reader.streams.en_in], \n",
    "                                     criterion.arguments[1]: mb_train[train_reader.streams.en_in_extended],\n",
    "                                     #criterion.arguments[2]: mb_train[train_reader.streams.de_in],\n",
    "                                     criterion.arguments[2]: mb_train[train_reader.streams.target]})\n",
    "\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=False) # log progress\n",
    "\n",
    "            # every N MBs evaluate on a test sequence to visually show how we're doing\n",
    "            #if mbs % eval_freq == 0: \n",
    "                #mb_valid = valid_reader.next_minibatch(1)\n",
    "\n",
    "                # run an eval on the decoder output model (i.e. don't use the groundtruth)\n",
    "                #e = model_greedy(mb_valid[valid_reader.streams.features])\n",
    "                #print(format_sequences(sparse_to_dense(mb_valid[valid_reader.streams.features]), i2w))\n",
    "                #print(\"->\")\n",
    "                #print(format_sequences(e, i2w))\n",
    "\n",
    "                # visualizing attention window\n",
    "                #if use_attention:\n",
    "                    #debug_attention(model_greedy, mb_valid[valid_reader.streams.features])\n",
    "\n",
    "            total_samples += mb_train[train_reader.streams.en_in].num_samples\n",
    "            #mbs += 1\n",
    "\n",
    "        # log a summary of the stats for the epoch\n",
    "        progress_printer.epoch_summary(with_metric=False)\n",
    "\n",
    "    # done: save the final model\n",
    "    model_path = \"model_%d.cmf\" % epoch\n",
    "    print(\"Saving final model to '%s'\" % model_path)\n",
    "    s2smodel.save(model_path)\n",
    "    print(\"%d epochs complete.\" % max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    embed1 = C.layers.Embedding(embedding_dim, name='embed1')\n",
    "    embed2 = C.layers.Embedding(embedding_dim, name='embed2')\n",
    "    \n",
    "    with C.layers.default_options(enable_self_stabilization=True):\n",
    "        encode = C.layers.Sequential([\n",
    "            embed1,\n",
    "            C.layers.Stabilizer(),\n",
    "            (C.layers.Recurrence(C.layers.LSTM(hidden_dim),return_full_state=True),C.layers.Recurrence(C.layers.LSTM(hidden_dim),return_full_state=True)),\n",
    "        ])\n",
    "\n",
    "    with C.layers.default_options(enable_self_stabilization=True):\n",
    "        # sub-layers\n",
    "        stab_in = C.layers.Stabilizer()\n",
    "        stab_out = C.layers.Stabilizer()\n",
    "        proj_out = C.layers.Dense(vocab_dim, name='out_proj')\n",
    "        h_dense = C.layers.Dense(hidden_dim,activation=C.relu, name='en2de_h')\n",
    "        c_dense = C.layers.Dense(hidden_dim,activation=C.relu, name='en2de_c')\n",
    "        rec_block = C.layers.LSTM(hidden_dim)\n",
    "        attention_model = C.layers.AttentionModel(attention_dim, name='attention_model')\n",
    "        pgen_h_att = C.layers.Dense(1,activation=None)\n",
    "        pgen_h = C.layers.Dense(1,activation=None)\n",
    "        pgen_x = C.layers.Dense(1,activation=None)\n",
    "        \n",
    "        @C.Function\n",
    "        def decode(history, input, extended_input):\n",
    "            encoded_input = encode(input)\n",
    "            encoded_h = C.splice(encoded_input[0],encoded_input[2])\n",
    "            encoded_h = h_dense(encoded_h)\n",
    "            encoded_c = C.splice(encoded_input[1][-1],encoded_input[3][-1])\n",
    "            encoded_c = c_dense(encoded_c)\n",
    "            x = embed2(history)\n",
    "            x = stab_in(x)\n",
    "            r = C.layers.RecurrenceFrom(rec_block,return_full_state=True)(encoded_h,encoded_c,x)\n",
    "            h_att = attention_model(encoded_input.outputs[0], r[0])\n",
    "            pgen = C.sigmoid(pgen_h_att(h_att)+pgen_h(r[0])+pgen_x(x))\n",
    "            att_w = h_att.attention_weights\n",
    "            tmp = C.sequence.broadcast_as(C.sequence.unpack(extended_input,0,no_mask_output=True),att_w)\n",
    "            att_dist = C.reduce_sum(C.element_times(att_w,tmp,name='get_att_dist'),axis=0,name='att_dist')\n",
    "            att_dist =  C.reshape(att_dist, (), 0, 1)\n",
    "            voc_dist = stab_out(C.splice(r[0],h_att))\n",
    "            voc_dist = proj_out(voc_dist)\n",
    "            voc_dist = C.layers.Label('voc_proj_out')(voc_dist)\n",
    "            voc_dist = C.pad(voc_dist, pattern=[(0,400)],mode=C.ops.CONSTANT_PAD, constant_value=0)\n",
    "            extend_dist = C.element_times(pgen,voc_dist,name='p_voc_dist') + C.element_times((1-pgen),att_dist,name='p_att_dist')\n",
    "            return (extend_dist,att_dist,att_w,tmp)\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(Sequence::Slice): Input('target', [#, deAxis], [50404]) -> Output('Block26298_Output_0', [#, deAxis_times_1_minus_1], [50404])\n",
      "Composite(Negate): Input('target', [#, deAxis], [50404]), Input('input', [#, enAxis], [50004]), Input('extended_input', [#, enAxis], [50404]) -> Output('Negate28674_Output_0', [#, deAxis_times_1_minus_1], [1])\n",
      "Composite(Sequence::Slice): Input('input', [#, enAxis], [50004]), Input('extended_input', [#, enAxis], [50404]), Input('target', [#, deAxis], [50404]) -> Output('Block28712_Output_0', [#, deAxis_times_1_minus_1], [50404])\n",
      "Composite(Negate): Input('input', [#, enAxis], [50004]), Input('extended_input', [#, enAxis], [50404]), Input('target', [#, deAxis], [50404]) -> Output('Negate31094_Output_0', [#, deAxis_times_1_minus_1], [1])\n",
      "Training 16634981 parameters in 36 parameter tensors.\n",
      "\n",
      "(1, 554, 50004)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA failure 2: out of memory ; GPU=0 ; hostname=GCRGDW132 ; expr=cudaMalloc((void**) &deviceBufferPtr, sizeof(AllocatedElemType) * AsMultipleOf(numElements, 2))\n\n[CALL STACK]\n    > Microsoft::MSR::CNTK::CudaTimer::  Stop\n    - Microsoft::MSR::CNTK::CudaTimer::  Stop (x2)\n    - Microsoft::MSR::CNTK::GPUMatrix<float>::  Resize\n    - Microsoft::MSR::CNTK::Matrix<float>::  Resize\n    - Microsoft::MSR::CNTK::TracingGPUMemoryAllocator::  operator= (x4)\n    - CNTK::Internal::  UseSparseGradientAggregationInDataParallelSGD\n    - CNTK::  CreateTrainer\n    - CNTK::Trainer::  TotalNumberOfUnitsSeen\n    - CNTK::Trainer::  TrainMinibatch (x2)\n    - PyInit__cntk_py (x2)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-83f377f9cc7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m160000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-2d9ed94465b2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_reader, valid_reader, vocab, i2w, s2smodel, max_epochs, epoch_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m                                      \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmb_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men_in_extended\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                                      \u001b[1;31m#criterion.arguments[2]: mb_train[train_reader.streams.de_in],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                                      criterion.arguments[2]: mb_train[train_reader.streams.target]})\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mprogress_printer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_with_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# log progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\cntk\\train\\trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontains_minibatch_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 updated = super(Trainer, self).train_minibatch_overload_for_minibatchdata(\n\u001b[1;32m--> 181\u001b[1;33m                     arguments, device)\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\cntk\\cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch_overload_for_minibatchdata\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2849\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch_overload_for_minibatchdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2850\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer_train_minibatch_overload_for_minibatchdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2852\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA failure 2: out of memory ; GPU=0 ; hostname=GCRGDW132 ; expr=cudaMalloc((void**) &deviceBufferPtr, sizeof(AllocatedElemType) * AsMultipleOf(numElements, 2))\n\n[CALL STACK]\n    > Microsoft::MSR::CNTK::CudaTimer::  Stop\n    - Microsoft::MSR::CNTK::CudaTimer::  Stop (x2)\n    - Microsoft::MSR::CNTK::GPUMatrix<float>::  Resize\n    - Microsoft::MSR::CNTK::Matrix<float>::  Resize\n    - Microsoft::MSR::CNTK::TracingGPUMemoryAllocator::  operator= (x4)\n    - CNTK::Internal::  UseSparseGradientAggregationInDataParallelSGD\n    - CNTK::  CreateTrainer\n    - CNTK::Trainer::  TotalNumberOfUnitsSeen\n    - CNTK::Trainer::  TrainMinibatch (x2)\n    - PyInit__cntk_py (x2)\n\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "train(train_reader, train_reader, vocab, i2w, model, max_epochs=1, epoch_size=160000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
